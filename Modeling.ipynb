{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling\n",
    "\n",
    "**Wrangle**\n",
    "\n",
    "1. Acquired data from student-mat.csv. \n",
    "\n",
    "2. Create dummy vars\n",
    "\n",
    "3. Split data \n",
    "\n",
    "4. Scale data\n",
    "\n",
    "wrangle.wrangle_sutdent_math(path) returns the following: \n",
    "\n",
    "| Object Returned | Description | Purpose                          |\n",
    "|:-------------------|:--------------------------------|:----------------------------------------------------|\n",
    "| 1. df | **Dataframe**, **Feature** and **target** variables, **Unscaled**, Dummy vars **with** original categorical vars | New features, additional cleaning needed, etc. |\n",
    "| 2. X_train_exp | **Dataframe**, **Feature** variables only, **Unscaled**, Dummy vars **with** original categorical vars | Exploration & analysis     |\n",
    "| 3. X_train | **Dataframe**, **Feature** variables only, **Scaled**, Dummy vars **without** original categorical vars | Feature selection, fit models, make predictions |\n",
    "| 4. y_train | **Series**, **Target** variable only, **Unscaled** | Feature selection, evaluate model predictions |\n",
    "| 5. X_validate | **Dataframe**, **Features** variables only, **Scaled**, Dummy vars **without** original categorical vars | Make predictions using top models |\n",
    "| 6. y_validate | **Series**, **Target** variable only, **Unscaled** | Evaluate model predictions made from X_validate to assess overfitting | \n",
    "| 7. X_test | **Dataframe**, **Features** variables only, **Scaled**, Dummy vars **without** original categorical vars | Make predictions using best model|\n",
    "| 8. y_test | **Series**, **Target** variable only, **Unscaled** | Evaluate model predictions made from X_test to estimate future performance on new data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling methods\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, LassoLars, TweedieRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import explained_variance_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='https://gist.githubusercontent.com/ryanorsinger/55ccfd2f7820af169baea5aad3a9c60d/raw/da6c5a33307ed7ee207bd119d3361062a1d1c07e/student-mat.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, \\\n",
    "X_train_exp, \\\n",
    "X_train, \\\n",
    "y_train, \\\n",
    "X_validate, \\\n",
    "y_validate, \\\n",
    "X_test, \\\n",
    "y_test = wrangle.wrangle_student_math(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many observations and features we have in each dataframe\n",
    "X_train.shape, X_validate.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the X_train head:\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Variable/y**\n",
    "\n",
    "This helps us determine which type of algorithm we may want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we determine distribution?\n",
    "plt.hist(y_train)\n",
    "plt.title('Distribution of Target(G3)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Baseline\n",
    "\n",
    "About the initial baseline: \n",
    "\n",
    "> Before we begin making models, we need to know how well we can estimate (predict) the final grade (G3) without using any features. This is often done by predicting every observation's target value to be the mean or the median. E.g. we could predict every student's final grade to be the mean final grade of all the students in our training sample. We will try both the mean and the median, see which performs best, and set that evaluation metric value as our baseline performance to beat. \n",
    "\n",
    "\n",
    "1. Predict all final grades to be 10.52, which is equal to the mean of G3 for the training sample. Store in `y_train['G3_pred_mean']`. \n",
    "\n",
    "2. Predict all final grades to be 11, which is equal to the median of G3 for the training sample. Store in `y_train['G3_pred_median']`.  \n",
    "\n",
    "3. Compute the RMSE comparing actual final grade (G3) to G3_pred_mean. \n",
    "\n",
    "4. Compute the RMSE comparing actual final grade (G3) to G3_pred_median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need y_train and y_validate to be dataframes to append the new columns with predicted values. \n",
    "y_train = pd.DataFrame(y_train)\n",
    "y_validate = pd.DataFrame(y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Predict G3_pred_mean\n",
    "G3_pred_mean = y_train.G3.mean()\n",
    "y_train['G3_pred_mean'] = G3_pred_mean\n",
    "y_validate['G3_pred_mean'] = G3_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. compute G3_pred_median\n",
    "G3_pred_median = y_train.G3.median()\n",
    "y_train['G3_pred_median'] = G3_pred_median\n",
    "y_validate['G3_pred_median'] = G3_pred_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. RMSE of G3_pred_mean\n",
    "rmse_train = mean_squared_error(y_train.G3,\n",
    "                                y_train.G3_pred_mean) ** .5\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_mean) ** (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE using Mean\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. RMSE of G3_pred_median\n",
    "rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_median) ** .5\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_median) ** (0.5)\n",
    "print(\"RMSE using Median\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Addendum/Note:\n",
    "we will incrementally build \n",
    "a dataframe for comparison of \n",
    "our metrics for model selection\n",
    "for ease of reflection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = pd.DataFrame(data=[\n",
    "            {\n",
    "                'model': 'mean_baseline', \n",
    "                'RMSE_train': rmse_train,\n",
    "                'RMSE_validate': rmse_validate\n",
    "                }\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n",
    "plt.hist(y_train.G3, color='blue', alpha=.5, label=\"Actual Final Grades\")\n",
    "plt.hist(y_train.G3_pred_mean, bins=1, color='red', alpha=.5,  label=\"Predicted Final Grades - Mean\")\n",
    "plt.hist(y_train.G3_pred_median, bins=1, color='orange', alpha=.5, label=\"Predicted Final Grades - Median\")\n",
    "plt.xlabel(\"Final Grade (G3)\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression (OLS)\n",
    "\n",
    "1. Fit the model using X_train (scaled) and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (lm). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "# \n",
    "# make the thing\n",
    "# \n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "# \n",
    "# fit the thing\n",
    "# \n",
    "lm.fit(X_train, y_train.G3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict train\n",
    "# \n",
    "# use the thing!\n",
    "# \n",
    "y_train['G3_pred_lm'] = lm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_lm) ** (1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['G3_pred_lm'] = lm.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_lm) ** (1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = metric_df.append({\n",
    "    'model': 'OLS Regressor', \n",
    "    'RMSE_train': rmse_train,\n",
    "    'RMSE_validate': rmse_validate,\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the coefficeints of the linear equation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoLars\n",
    "\n",
    "1. Fit the model using X_train (scaled) and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (lars). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lars = LassoLars(alpha=0.01)\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series!\n",
    "lars.fit(X_train, y_train.G3)\n",
    "\n",
    "# predict train\n",
    "y_train['G3_pred_lars'] = lars.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_lars) ** (1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['G3_pred_lars'] = lars.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_lars) ** (1/2)\n",
    "\n",
    "print(\"RMSE for Lasso + Lars\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate,\n",
    "     \"\\nDifference: \", rmse_validate - rmse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = metric_df.append({\n",
    "    'model': 'lasso_alpha0.01', \n",
    "    'RMSE_train': rmse_train,\n",
    "    'RMSE_validate': rmse_validate,\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black; border-radius: 3px; background: green; padding: .5em 1em;\">\n",
    "    <p style=\"font-size: 1.3em; font-weight: bold\">Mini Exercise</p>\n",
    "    <ol>\n",
    "        <li>Use a different <code>alpha</code> with the lasso model to try the fit with a different penalty based on the above code.</li>\n",
    "        <li>What do you notice about the difference in rmse between train and validate sets?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweedieRegressor (GLM)\n",
    "\n",
    "1. Fit the model using X_train (scaled) and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (glm). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TweedieRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.G3.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "glm = TweedieRegressor(power=1, alpha=0)\n",
    "\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "glm.fit(X_train, y_train.G3)\n",
    "\n",
    "# predict train\n",
    "y_train['G3_pred_glm'] = glm.predict(X_train)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_glm) ** (1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['G3_pred_glm'] = glm.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_glm) ** (1/2)\n",
    "\n",
    "print(\"RMSE for GLM using Tweedie, power=0 & alpha=0\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black; border-radius: 3px; background: green; padding: .5em 1em;\">\n",
    "    <p style=\"font-size: 1.3em; font-weight: bold\">Mini Exercise</p>\n",
    "    <ol>\n",
    "        <li><code>Use a different power</code> with the generalized linear model to try the fit of a different distribution based on the above code.</li>\n",
    "        <li>Does it seem to perform better or worse than the first pick? Could you see why it may change? What does \"power\" mean here?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = metric_df.append({\n",
    "    'model': 'glm_poisson', \n",
    "    'RMSE_train': rmse_train,\n",
    "    'RMSE_validate': rmse_validate,\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Using sklearn.preprocessing.PolynommialFeatures() + sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the new features, based on value indicated for degree for train, validate & test. \n",
    "\n",
    "2. Fit the Linear Regression model\n",
    "\n",
    "3. Predict using the transformed (squared or cubed, e.g.) features \n",
    "\n",
    "4. Evaluate using RMSE\n",
    "\n",
    "5. Repeat predictions and evaluation for validation.\n",
    "\n",
    "6. Compare RMSE train vs. validation. Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PolynomialFeatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# fit and transform X_train_scaled\n",
    "X_train_degree2 = pf.fit_transform(X_train)\n",
    "\n",
    "# transform X_validate_scaled & X_test_scaled\n",
    "X_validate_degree2 = pf.transform(X_validate)\n",
    "X_test_degree2 =  pf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LinearRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm2 = LinearRegression()\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "lm2.fit(X_train_degree2, y_train.G3)\n",
    "\n",
    "# predict train\n",
    "y_train['G3_pred_lm2'] = lm2.predict(X_train_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_lm2) ** (1/2)\n",
    "\n",
    "# predict validate\n",
    "y_validate['G3_pred_lm2'] = lm2.predict(X_validate_degree2)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_lm2) ** 0.5\n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df = metric_df.append({\n",
    "    'model': 'quadratic', \n",
    "    'RMSE_train': rmse_train,\n",
    "    'RMSE_validate': rmse_validate,\n",
    "    }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "**Plotting Actual vs. Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(y_validate.G3, y_validate.G3_pred_mean, alpha=.5, color=\"gray\", label='_nolegend_')\n",
    "plt.annotate(\"Baseline: Predict Using Mean\", (16, 9.5))\n",
    "plt.plot(y_validate.G3, y_validate.G3, alpha=.5, color=\"blue\", label='_nolegend_')\n",
    "plt.annotate(\"The Ideal Line: Predicted = Actual\", (.5, 3.5), rotation=15.5)\n",
    "\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm, \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_glm, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm2, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Final Grade\")\n",
    "plt.ylabel(\"Predicted Final Grade\")\n",
    "plt.title(\"Where are predictions more extreme? More modest?\")\n",
    "# plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "# plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual Plots: Plotting the Errors in Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axhline(label=\"No Error\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm - y_validate.G3 , \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_glm - y_validate.G3, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(y_validate.G3, y_validate.G3_pred_lm2 - y_validate.G3, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Final Grade\")\n",
    "plt.ylabel(\"Residual/Error: Predicted Grade - Actual Grade\")\n",
    "plt.title(\"Do the size of errors change as the actual value changes?\")\n",
    "plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(y_validate.G3, color='blue', alpha=.5, label=\"Actual Final Grades\")\n",
    "plt.hist(y_validate.G3_pred_lm, color='red', alpha=.5, label=\"Model: LinearRegression\")\n",
    "plt.hist(y_validate.G3_pred_glm, color='yellow', alpha=.5, label=\"Model: TweedieRegressor\")\n",
    "plt.hist(y_validate.G3_pred_lm2, color='green', alpha=.5, label=\"Model 2nd degree Polynomial\")\n",
    "plt.xlabel(\"Final Grade (G3)\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.title(\"Comparing the Distribution of Actual Grades to Distributions of Predicted Grades for the Top Models\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addendum: Comparing models DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection & Out-of-Sample Evaluation**\n",
    "\n",
    "Model selected: lars (using Lasso lars with alpha = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(y_test)\n",
    "\n",
    "# predict on test\n",
    "y_test['G3_pred_lars'] = lars.predict(X_test)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_test = mean_squared_error(y_test.G3, y_test.G3_pred_lars) ** (1/2)\n",
    "\n",
    "print(\"RMSE for OLS Model using LinearRegression\\nOut-of-Sample Performance: \", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
